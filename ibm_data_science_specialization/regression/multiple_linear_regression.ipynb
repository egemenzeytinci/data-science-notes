{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordinary Least Squares (OLS)\n",
    "\n",
    " **Coefficient** and **Intercept**, are the parameters of the fit line. \n",
    "Given that it is a multiple linear regression, with 3 parameters, and knowing that the parameters are the intercept and coefficients of hyperplane, sklearn can estimate them from our data. Scikit-learn uses plain Ordinary Least Squares method to solve this problem.\n",
    "\n",
    "OLS is a method for estimating the unknown parameters in a linear regression model. OLS chooses the parameters of a linear function of a set of explanatory variables by minimizing the sum of the squares of the differences between the target dependent variable and those predicted by the linear function. In other words, it tries to minimizes the sum of squared errors (SSE) or mean squared error (MSE) between the target variable (y) and our predicted output (yhat) over all samples in the dataset.\n",
    "\n",
    "OLS can find the best parameters using of the following methods:\n",
    "\n",
    "```\n",
    "- Solving the model parameters analytically using closed-form equations\n",
    "- Using an optimization algorithm (Gradient Descent, Stochastic Gradient Descent, Newtonâ€™s Method, etc.)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/fuel_consumption_co2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    'ENGINESIZE',\n",
    "    'CYLINDERS',\n",
    "    'FUELCONSUMPTION_CITY',\n",
    "    'FUELCONSUMPTION_HWY',\n",
    "    'FUELCONSUMPTION_COMB',\n",
    "    'CO2EMISSIONS'\n",
    "]\n",
    "cdf = df[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "msk = np.random.rand(len(df)) < 0.8\n",
    "train = cdf[msk]\n",
    "test = cdf[~msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "independents = [\n",
    "    'ENGINESIZE',\n",
    "    'CYLINDERS',\n",
    "    'FUELCONSUMPTION_CITY',\n",
    "    'FUELCONSUMPTION_HWY'\n",
    "]\n",
    "dependent = ['CO2EMISSIONS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = LinearRegression()\n",
    "X_train = np.asanyarray(train[independents])\n",
    "y_train = np.asanyarray(train[dependent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients:  [[11.85159853  6.22788704  6.49985737  3.01636028]]\n"
     ]
    }
   ],
   "source": [
    "regr.fit(X_train, y_train)\n",
    "print('Coefficients: ', regr.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = regr.predict(test[independents])\n",
    "X_test = np.asanyarray(test[independents])\n",
    "y_test = np.asanyarray(test[dependent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residual sum of squares: 602.09\n",
      "Variance score: 0.84\n"
     ]
    }
   ],
   "source": [
    "print('Residual sum of squares: %.2f' % np.mean((y_hat - y_test) ** 2))\n",
    "print('Variance score: %.2f' % regr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "When the number of rows in your data set is less than 10,000, you can think of OLS as an option. However, for greater values, you should try other faster approaches. The second option is to use an optimization algorithm to find the best parameters. That is, you can use a process of optimizing the values of the coefficients by iteratively minimizing the error of the model on your training data. For example, you can use gradient descent which starts optimization with random values for each coefficient, then calculates the errors and tries to minimize it through y's changing of the coefficients in multiple iterations. Gradient descent is a proper approach if you have a large data set. Please understand however, that there are other approaches to estimate the parameters of the multiple linear regression that you can explore on your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize independent variables\n",
    "X_norm = X_train.copy()\n",
    "\n",
    "min_x = np.min(X_train)\n",
    "max_x = np.max(X_train)\n",
    "\n",
    "X_norm = (X_train - min_x) / (max_x - min_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize dependent variables\n",
    "y_norm = y_train.copy()\n",
    "\n",
    "max_y = np.max(y_train)\n",
    "min_y = np.min(y_train)\n",
    "\n",
    "y_norm = (y_train - min_y) / (max_y - min_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(theta, X, y):\n",
    "    return 1 / M * np.sum((X.dot(theta) - y) * X, axis=0).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_func(theta, X, y):\n",
    "    return np.sum((X.dot(theta) - y) ** 2, axis=0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(theta0, X, y, learning_rate=0.5, epochs=1000, TOL=1e-7):\n",
    "    theta_history = [theta0]\n",
    "    j_history = [cost_func(theta0, X, y)]\n",
    "    \n",
    "    theta_new = theta0 * 10000\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(f'{epoch:5d}\\t{j_history[-1]:7.4f}\\t')\n",
    "            \n",
    "        dj = grad(theta0, X, y)\n",
    "        j = cost_func(theta0, X, y)\n",
    "        \n",
    "        theta_new = theta0 - learning_rate * dj\n",
    "        theta_history.append(theta_new)\n",
    "        j_history.append(j)\n",
    "        \n",
    "        if np.sum((theta_new - theta0) ** 2) < TOL:\n",
    "            print('Convergence achieved.')\n",
    "            break\n",
    "            \n",
    "        theta0 = theta_new\n",
    "\n",
    "    return theta_new, theta_history, j_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, theta):\n",
    "    X_norm = (X - min_x) / (max_x - min_x)\n",
    "    y_pred_norm = X_norm.dot(theta)\n",
    "    \n",
    "    return y_pred_norm * (max_y - min_y) + min_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta0 = np.zeros((X_norm.shape[1], 1)) + 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0\t 3.8723\t\n",
      "  100\t 3.6385\t\n",
      "  200\t 3.5909\t\n",
      "  300\t 3.5504\t\n",
      "Convergence achieved.\n"
     ]
    }
   ],
   "source": [
    "theta, theta_history, j_history = gradient_descent(theta0, X_norm, y_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = predict(X_test, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residual sum of squares: 626.66\n"
     ]
    }
   ],
   "source": [
    "print('Residual sum of squares: %.2f' % np.mean((y_hat - y_test) ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More information about gradient descent method is [here.](https://euanrussano.github.io/20190810linearRegressionNumpy/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
