{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RDD\n",
    "This is the most basic data abstraction in Spark, short for resilient distributed dataset. It is a fault-tolerant collection of elements that can be operated on in parallel.\n",
    "- Most basic data structure in Spark\n",
    "- Fault tolerant\n",
    "- Parallel\n",
    "- Immutable\n",
    "- Low level transformation API (does not recommend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T19:34:04.787297Z",
     "start_time": "2019-11-17T19:33:41.089423Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://ytuegemtincimbp:4040\n",
       "SparkContext available as 'sc' (version = 2.4.4, master = local[*], app id = local-1574019237303)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "r: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:25\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val r = sc.parallelize(Array(1, 2, 3, 4, 5)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T19:34:24.481563Z",
     "start_time": "2019-11-17T19:34:23.575511Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "filteredList: List[Int] = List(4, 5)\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val filteredList = r.filter(k => k > 3).collect.toList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataFrame\n",
    "It is a distributed collection of data organized into named columns. It is conceptually equivalent to a table in a relational database or a data frame but with lot more stuff under the hood.\n",
    "- Structured data structure\n",
    "- Written top of RDD API\n",
    "- Faster than RDD\n",
    "- Easy to use\n",
    "- High level transformation API\n",
    "- Support SQL API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T19:34:27.372716Z",
     "start_time": "2019-11-17T19:34:25.683803Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "r: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[2] at parallelize at <console>:27\n",
       "df: org.apache.spark.sql.DataFrame = [value: int]\n",
       "filteredDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [value: int]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val r = sc.parallelize(Array(1, 2, 3, 4, 5)) \n",
    "val df = r.toDF\n",
    "val filteredDF = df.where(col(\"value\") > 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T19:34:28.416085Z",
     "start_time": "2019-11-17T19:34:27.522981Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "|    5|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filteredDF.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T19:48:54.708636Z",
     "start_time": "2019-11-17T19:48:53.526529Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [cluster: string, number: string]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.read.option(\"header\", \"True\").csv(\"file:///tmp/example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T19:49:02.031593Z",
     "start_time": "2019-11-17T19:49:01.734436Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|cluster|number|\n",
      "+-------+------+\n",
      "|      c|   862|\n",
      "|      d|   225|\n",
      "|      e|   524|\n",
      "|      d|   643|\n",
      "|      b|   628|\n",
      "+-------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T19:49:55.219240Z",
     "start_time": "2019-11-17T19:49:54.863330Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "g: org.apache.spark.sql.DataFrame = [cluster: string, m: string]\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val g = df.groupBy(col(\"cluster\")).agg(max(col(\"number\")).alias(\"m\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T19:50:57.221097Z",
     "start_time": "2019-11-17T19:50:53.342403Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|cluster|  m|\n",
      "+-------+---+\n",
      "|      e|999|\n",
      "|      d|999|\n",
      "|      c|999|\n",
      "|      b|999|\n",
      "|      a|999|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "g.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T20:00:56.533210Z",
     "start_time": "2019-11-17T20:00:51.950482Z"
    }
   },
   "outputs": [],
   "source": [
    "g.repartition(1).write.csv(\"file:///tmp/output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Join in Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T20:41:32.855898Z",
     "start_time": "2019-11-17T20:41:32.291697Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: string, name: string]\n",
       "b: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: string, age: string]\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val a = spark.read.option(\"header\", \"true\").csv(\"file:///tmp/adata/\").as(\"a\")\n",
    "val b = spark.read.option(\"header\", \"true\").csv(\"file:///tmp/bdata/\").as(\"b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T20:41:34.245476Z",
     "start_time": "2019-11-17T20:41:34.014879Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "c: org.apache.spark.sql.DataFrame = [id: string, name: string ... 1 more field]\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val c = a.join(b, col(\"a.id\") === col(\"b.id\")).select(\"a.id\", \"name\", \"age\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T20:41:35.837805Z",
     "start_time": "2019-11-17T20:41:35.480410Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+\n",
      "| id|name|age|\n",
      "+---+----+---+\n",
      "|752| Max| 39|\n",
      "|752| Max| 37|\n",
      "|736|John| 36|\n",
      "|677|Ivan| 30|\n",
      "|677|Ivan| 36|\n",
      "+---+----+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "c.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T20:42:07.805675Z",
     "start_time": "2019-11-17T20:42:07.646206Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "crossA: org.apache.spark.sql.DataFrame = [id: string, name: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val crossA = a.crossJoin(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T20:42:17.084589Z",
     "start_time": "2019-11-17T20:42:16.308530Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+---+\n",
      "| id|name| id|age|\n",
      "+---+----+---+---+\n",
      "|752| Max|752| 37|\n",
      "|752| Max|736| 36|\n",
      "|752| Max|677| 36|\n",
      "|752| Max| 14| 44|\n",
      "|752| Max|657| 29|\n",
      "+---+----+---+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "crossA.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several join types in Spark,\n",
    "- Broadcast hash join\n",
    "- Sort-merge join\n",
    "- Shuffle join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Broadcast hash join**,\n",
    "- Provides best performance\n",
    "- Suitable if both dataset is fit in the memory\n",
    "- Create hashtable for the key lookup\n",
    "- spark.sql.autoBroadcastJoinThreshold is the threshold for memory fit\n",
    "\n",
    "**Sort-merge join**,\n",
    "- May slower than hash join\n",
    "- Suitable if both dataset is cannot fit in the memory\n",
    "- Sort data before joining\n",
    "- spark.sql.join.preferSortMergeJoin is true by default\n",
    "\n",
    "**Shuffle join**,\n",
    "- May slower than hash join\n",
    "- May faster than sort-merge join\n",
    "- Partitioned broadcast join\n",
    "- spark.sql.join.preferSortMergeJoin=false and spark.sql.autoBroadcastJoinThreshold may need some tweak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benchmark,\n",
    "\n",
    "Join example 2 tables,\n",
    "\n",
    "```\n",
    "+--------+-------------+---------+\n",
    "| table  | records     | columns |\n",
    "+--------+-------------+---------+\n",
    "| users  | 1 million   | 4       |\n",
    "+--------+-------------+---------+\n",
    "| orders | 10 millions | 4       |\n",
    "+--------+-------------+---------+\n",
    "```\n",
    "\n",
    "Here is result,\n",
    "\n",
    "```\n",
    "+----------------+-------------+-------------+\n",
    "| type           | time (secs) | peak memory |\n",
    "+----------------+-------------+-------------+\n",
    "| Broadcast Hash | 20 seconds  | 424 MB      |\n",
    "+----------------+-------------+-------------+\n",
    "| Sort-Merge     | 8 seconds   | 4.2 MB      |\n",
    "+----------------+-------------+-------------+\n",
    "| Shuffle Hash   | 34 seconds  | 850 MB      |\n",
    "+----------------+-------------+-------------+\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
